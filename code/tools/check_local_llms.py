# list_hf_cache.py
import os
import csv
from pathlib import Path
from typing import Iterable, Optional
from huggingface_hub import scan_cache_dir, __version__ as HF_VER

def bytes2human(n: int) -> str:
    units = ["B","KB","MB","GB","TB"]
    i = 0
    f = float(n)
    while f >= 1024 and i < len(units)-1:
        f /= 1024.0
        i += 1
    return f"{f:.2f} {units[i]}"

def _as_iter(x) -> Iterable:
    if not x:
        return []
    # frozenset / list / tuple éƒ½èƒ½è¿­ä»£
    return x

def list_hf_cached_models(cache_dir: Optional[str] = None, csv_out: Optional[str] = None):
    info = scan_cache_dir(cache_dir=cache_dir)

    # æœ‰çš„ç‰ˆæœ¬æ²¡æœ‰ cache_dirï¼›æˆ‘ä»¬å°±æ˜¾ç¤ºå…¥å‚æˆ–é»˜è®¤æ¨æ–­
    cache_root = cache_dir or os.environ.get("HF_HOME") or os.environ.get("HF_HUB_CACHE") or "~/.cache/huggingface/hub"
    print(f"ğŸ¤— huggingface_hub = {HF_VER}")
    print(f"ğŸ—‚ Cache root (guessed): {cache_root}")

    repos = getattr(info, "repos", [])  # å…¼å®¹ï¼šæœ‰çš„ç‰ˆæœ¬å« repos
    print(f"ğŸ“¦ å·²ç¼“å­˜çš„ä»“åº“æ•°é‡: {len(repos)}\n")

    rows = []
    grand_bytes = 0

    # ä¸ºäº†ç¨³å®šè¾“å‡ºé¡ºåºï¼ŒæŒ‰ (repo_type, repo_id) æ’åºï¼ˆç¼ºå¤±å­—æ®µç»™ç©ºä¸²ï¼‰
    def _key(r):
        return (getattr(r, "repo_type", "") or "", getattr(r, "repo_id", "") or "")
    for repo in sorted(repos, key=_key):
        repo_id = getattr(repo, "repo_id", "UNKNOWN")
        repo_type = getattr(repo, "repo_type", "model")

        file_count = 0
        bytes_on_disk = 0
        seen_blobs = set()
        file_paths_for_prefix = []

        for rev in _as_iter(getattr(repo, "revisions", [])):
            files = getattr(rev, "files", [])  # éƒ¨åˆ†ç‰ˆæœ¬æ˜¯è¿™ä¸ªå­—æ®µ
            # æ—©æœŸç‰ˆæœ¬å¯èƒ½å« "files_on_disk" æˆ–ç±»ä¼¼ï¼›åšä¸ªå…œåº•
            if not files:
                files = getattr(rev, "files_on_disk", [])
            for f in _as_iter(files):
                file_count += 1
                # ä¼˜å…ˆç”¨ blob_path åšå»é‡ï¼›æ²¡æœ‰å°±é€€å› file_path
                blob = getattr(f, "blob_path", None) or getattr(f, "lfs_path", None) or getattr(f, "file_path", None)
                if blob and blob not in seen_blobs:
                    seen_blobs.add(blob)
                    size = getattr(f, "size_on_disk", None)
                    if isinstance(size, int):
                        bytes_on_disk += size
                    else:
                        # å†é€€å›ç”¨å®é™…æ–‡ä»¶å¤§å°ï¼ˆå¦‚æœ file_path å­˜åœ¨ï¼‰
                        fp = getattr(f, "file_path", None)
                        if fp and os.path.exists(fp):
                            try:
                                bytes_on_disk += os.path.getsize(fp)
                            except OSError:
                                pass
                # æ”¶é›† file_path ç”¨æ¥æ¨æ–­æœ¬åœ°è·¯å¾„å‰ç¼€ï¼ˆå±•ç¤ºç”¨ï¼‰
                fp = getattr(f, "file_path", None)
                if fp:
                    file_paths_for_prefix.append(fp)

        # å°è¯•æ¨æ–­ä»“åº“æœ¬åœ°è·¯å¾„ï¼ˆå…¬å…±å‰ç¼€ï¼‰
        repo_path = ""
        if file_paths_for_prefix:
            try:
                repo_path = os.path.commonpath(file_paths_for_prefix)
            except ValueError:
                # ä¸åŒæŒ‚è½½ç‚¹å¯èƒ½å¯¼è‡´ commonpath å¤±è´¥ï¼Œä¿æŒä¸ºç©ºå³å¯
                repo_path = ""

        grand_bytes += bytes_on_disk

        print(f"ğŸ”¹ Repo ID: {repo_id}")
        print(f"   â”œâ”€ ç±»å‹: {repo_type}")
        if repo_path:
            print(f"   â”œâ”€ æœ¬åœ°è·¯å¾„(æ¨æ–­): {repo_path}")
        print(f"   â”œâ”€ æ–‡ä»¶æ•°(å«å¤š revision): {file_count}")
        print(f"   â””â”€ å ç”¨ç©ºé—´(å»é‡å): {bytes2human(bytes_on_disk)}\n")

        rows.append({
            "repo_id": repo_id,
            "repo_type": repo_type,
            "repo_path_inferred": repo_path,
            "files_count_in_revisions": file_count,
            "unique_size_bytes": bytes_on_disk,
            "unique_size_human": bytes2human(bytes_on_disk),
        })

    print(f"ğŸ“Š æ±‡æ€»å ç”¨(æŒ‰å„ä»“åº“å†…éƒ¨å»é‡åç›¸åŠ ): {bytes2human(grand_bytes)}")

    if csv_out:
        csv_p = Path(csv_out)
        csv_p.parent.mkdir(parents=True, exist_ok=True)
        with open(csv_p, "w", newline="", encoding="utf-8") as f:
            fieldnames = ["repo_id","repo_type","repo_path_inferred","files_count_in_revisions","unique_size_bytes","unique_size_human"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for r in rows:
                writer.writerow(r)
        print(f"âœ… å·²å¯¼å‡º CSV: {csv_p}")

if __name__ == "__main__":
    # ç”¨é»˜è®¤ç¼“å­˜ï¼›å¦‚éœ€æŒ‡å®šï¼Œä¼  cache_dir="/home/you/.cache/huggingface/hub"
    list_hf_cached_models(csv_out="./hf_cache.csv")
